# Incident Response
![][../images/12/soc.jpg]

Information and cybersecurity can be summarized as detecting and preventing vulnerabilities and threats.  With the exception of the last chapter, much of this textbook has focused on the vulnerability side of infosec as vulnerabilities in their most general form provide the opportunity for threats to succeed.  This chapter will focus on how organizations prepare, identify and react to threats.  It is the second part of the detection and response section that expands on the previous Forensics and Malware Analysis chapter.  Readers will learn about the systems used to aggregate data and how threats are found within networks and systems.  We will also cover administrative efforts organizations place on planning for security incidents.

**Objectives**
1. Understand the role and procedures of a security operations center (SOC).
2. Install and configure a system information and event manager (SIEM) to detect threats across an organization using Splunk
3. Learn the major components and processes of and Incident Response plan.
4. Understand the role of Threat Hunting and how it identifies security incidents in an network.

## System Information and Event Manager (SIEM)
Most medium to large organizations will invest in a system that aggregates logs from systems in their network.  This **system information and event manager (SIEM)** enables analysts to search and filter system logs to identify events that occurred.  For instance, Windows Desktop logs fed into the SIEM will include logon events where an analyst could quickly and remotely identify who attempted logons to that system overtime.  Using these log events, the SIEM is also capable of using advanced queries that persist, or save, that can be configured to alert and notify analysts when a pattern of events arise.  Expanding on the previous example, an alert can be established within the SIEM that notifies analyst when a device has 5 consecutive failed logon attempts within a 1 minute window which could signal a brute force attack.  As you might have gathered, SIEMs are powerful and complex systems that enrich security teams *threat monitoring* on a network.  

>[!tip] Tip - Pronouncing SIEM
>Yet another acronym that has great debate surrounding its pronunciation.  Your author is in the camp of pronouncing it as "SIM" while many other professionals, especially outside the United States, pronounce it as "SEEM".  
### Security Operations Center (SOC)
While most organizations have a natural flow of activity usually centered around a work day, security never sleeps.  Some operations expand the globe or operate 24 hours a day which puts added pressure on security teams to always be monitoring the network.  Regardless, attackers and threat actors may become active at any time and security professionals must always be on guard should security events or incidents occur.  Given this need for multiple shifts of security professionals, where there is currently a shortage in the labor market, many medium sized organizations outsource the labor needed to manage a SIEM to companies called *managed security service providers (MSSP)*.  Such companies centralize and streamline security monitoring while maintain a talent pool of security professionals covering all daily shifts.  With enough customers MSSPs achieve and economy of scale medium sized organizations can't acquire on their own.

The entity at an organization responsible for maintaining, managing, and monitoring the SIEM, regardless of using an MSSP or insourcing within the company, is referred to as the **security operations center (SOC)**.  The SOC can be an loosely organized group of 1 or more individuals who use the SIEM for threat monitoring even as simple as an email distribution list.  It can also be a formal department with several teams with multiple offices that share dashboards on multiple projectors as is illustrated with this chapter's cover art (generated by DALLE).

>[!tip] Tip - Breaking into Cybersecurity
>SOCs are in constant need for new analysts as the work is laborious and many analysts quickly get promoted to other security roles.  Many individuals find SOC Analyst roles as their way of breaking into cybersecurity given this market demand.  Usually these jobs expect some IT experience, such as 1-2 years doing help desk or working as a junior systems administrator.

No matter how you go about it, a SOC is expensive to setup and maintain.  It requires procuring costly services from a third party or hiring specialized professionals.  Usually, one person is not enough because it is unfair, or in some jurisdictions illegal, to require someone to be available 24 hours a day and 7 days a week.  Aside from the monitoring and response efforts SOC team members must stay abreast of new threats and techniques while integrating them into their procedures.  In addition, the area experiences a high personnel turn over rate because the work is stressful and never ends.  Analyst are often rotated to other duties as they may experience *alert fatigue* where they make mistakes when handling security events, such as type II errors or missing a true positive causing a threat to go unheeded.
### Setting Up a SIEM
The following diagram illustrates a general setup for a SIEM within an organization's network.  This section will describe each node and how it is used so please take a moment to familiarize yourself with it.

![[../images/12/siem_arch.png|SIEM Architecture|600]]

The primary interface of a SIEM solution is a web application sometimes called a console.  Users of the system, such as SOC team members, log into the application through their browsers.  Ideally, this SIEM web application is only available to the devices for users who have a need to access it, and not available on the public internet.  Access to the system requires an account with valid credentials, username and password.  Once logged in the users can begin querying the logs and conducting investigations.

>[!info] Info - SIEM Users
>SIEMs are powerful tools that can assist more then security personnel.  There may be additional use cases for system administrators and even developers to use the SIEM to monitor and troubleshoot systems and applications health.

A SIEM's lifeblood is the data that is fed into it.  Without a constant live stream of log files from various systems there wouldn't be much relevant data for SOC analysts to query.  Almost any log that follows a structured data pattern can be sent to the SIEM over the network.  Usually administrators will want to feed as much as possible into the SIEM to have the most opportunity to identify malicious events.  This typically includes alerts from security systems such as *end point detection and response (EDR)*, antivirus, and IDS/IPS systems, as well as logs from servers, workstations, and network equipment like routers and firewalls.

Log data is sent to an *ingestor* in a *push*, where the node initiates the sending, or *pull* method, where the ingestor initiates the data transfer.  This collection of data should be over encrypted channels, like TLS, to ensure the data is not manipulated in transit.  Log ingestion usually uses authentication in addition to encryption to ensure only authorized devices send data.  This exchange could use *agentless* approaches where a service account accesses the respective endpoint over secure protocols like SSH.  A more robust and stable approach is to use an *agent*, which is a software application built for the SIEM's purposes, to exchange the data.  The ingestor will validate the sender and the data being consumed before preparing, or parsing, it for storage in a database.  SIEM administrators must consider how frequently to send the data to the ingestor.  Too frequent, such as every second, may cause network congestion or reach ingestor or database resource limitations.  However, running ingestions too far apart, such as once a day, degrades the SOC's ability to identify and respond to threats quickly and could also reach limitations with the amount of data being transferred at once.

>[!tip] Tip - Separation of Concerns
>The SIEM ingestor, database, and web server can all be installed on the same server but it is a best practice to separate each function onto its own server.  This provides flexibility with disaster recovery planning and troubleshooting potential issues.

Data sent to the database is stored to be used by the SIEM's web application.  Most solutions will align the timestamps of logs to coordinated universal time (UTC) and then present the relative time to the user's selected time zone preference within their account.  When a SIEM user queries data through the application, a call is made to the database to retrieve the searched information.  You can imagine that the database will be very busy writing new data in large quantities alongside handling user queries.  Most SIEM solutions will speed up the query process by creating *indexes* of the data at regular intervals, such as daily.  Another design decisions must be made surrounding the *retention period* of how long to store the data.  Data retained for too long may be needlessly expensive to support the indexing and storage costs while providing little value as it becomes less likely to be searched the older it gets.  Most organizations tend to use the retention period of one year, but this period could be higher or lower depending on the organization's needs.  

>[!activity] Activity 12.1 - Splunk Setup
> Splunk is a very popular SIEM solution that has all the modern features needed for security teams to identify and track threats in an environment.  Splunk offers a free license that has a limit of 500 MB ingestion a day and also excludes features such as alerting, user management, and data forwarding.  Having a basic understanding of how to setup and use the tool serves as the foundation many SOC analysts' careers are built from.  In this activity I will demonstrate the local installation and features of Splunk using the BOTSV3 dataset.
> 
> After starting the Ubuntu VM in Bridge Adapter network mode, I launch a browser and navigate to the Splunk Enterprise registration page https://www.splunk.com/en_us/download/splunk-enterprise.html.  The form on this page requires the use of a business email - I recommend university students use their `.edu` emails as these are typically accepted by Splunk.
> ![[../images/12/splunk_activity_register.png|Splunk Registration|600]]
> Upon logging in I reach the download page where I select the Linux tab and download the `.deb` installer.  This immediately takes me to the EULA acceptance page that I agree to and submit to commence the download.
> ![[../images/12/splunk_activity_download.png|Splunk DEB Download|600]]
> With the download completed, I open a terminal, update my system, install the dependency curl, using apt, then finally install the Splunk `.deb` file using dpkg.  The `.deb` file is a Debian repository file that contains instructions and files to install Splunk.
> ```bash
> sudo apt update -y
> sudo apt install curl -y
> sudo dpkg -i ~/Downloads/splunk*.deb
> ```
> ![[../images/12/splunk_activity_curl_install.png|Installing Curl on Ubuntu|600]]
> The installation takes a minute to complete as there are a lot of files to unpack!
> ![[../images/12/splunk_activity_splunk_install.png|Installing Splunk Using Dpkg|600]]
> As soon as Splunk's installation is completed, it can be started suing the Splunk binary stored in the `/opt/splunk/bin/` directory.  The initial start requires acceptance of the license agreement using the `q` and the `y` letters.  Additionally, the CLI requires entering the username and password to create a new user for web console access.
> ```bash
> sudo /opt/splunk/bin/splunk start
> ```
> ![[../images/12/splunk_activity_start.png|Startng Splunk|600]]
> ![[../images/12/splunk_activity_config.png|Accepting License and Creating User|600]]
> The installation completes in a few seconds and then returns me to the command prompt with a message to access Splunk over http://154-ubuntu:8000 which is the name of my virtual machine.
> ![[../images/12/splunk_activity_install_complete.png|Splunk Installation Complete|600]]
> I then open the browser in my VM and navigate to http://154-ubuntu:8000.  I'm presented with a login page where I enter my username and password I established during the installation of the software.
> ![[../images/12/splunk_activity_login.png|Local Splunk Instance Login Page|600]]
> Splunk isn't very useful without data so the next step is to populate the system with various data sources.  An organization would configure systems with agents and connections to continuously feed data into Splunk.  Because this is just a demonstration I will be using a prepared test data set published on Splunk's GitHub page.  I navigate to https://github.com/splunk/botsv3 and download the BOTSV3 Dataset which is a curated set of logs used in Splunk's Boss of the SOC capture the flag challenge.
> ![[../images/12/splunk_activity_bots_download.png|BOTSV3 Download Page|600]]
> After the download completes, which takes a few minutes, I move the `.tgz` file from the downloads folder into the `/opt/splunk/etc/apps/` directory.  Then I unzip the download using gunzip and unarchive the unzipped file using tar.
> ```bash
> sudo mv ~/Downloads/botsv3_data_set.tgz /opt/splunk/etc/apps/
> sudo gunzip /opt/splunk/etc/apps/botsv3_data_set.tgz
> sudo tar -xvf /opt/splunk/etc/apps/botsv3_data_set.tar -C  /opt/splunk/etc/apps/
> ```
> ![[../images/12/splunk_activity_extract_bots.png|Extracting BOTSV3 Dataset Into Splunk|600]]
> A restart of Splunk is required for the dataset to be recognized in the console.
> ```bash
> sudo /opt/splunk/bin/splunk restart
> ```
> ![[../images/12/splunk_activity_restart.png|Restarting Splunk|600]]
> Once restarted, I navigate back to the app (http://154-ubuntu:8000) and press the Apps dropdown on the top navigation bar and select "Search & Reporting".
> ![[../images/12/splunk_activity_search_view.png|Splunk Search View|400]]
> 
> Afterwards I enter the SPL, or search query, `index=botsv3` into the search bar and change the time scope to "All Time" since this dataset is older.  Millions of events are eventually loaded after a few minutes!
> ![[../images/12/splunk_activity_init_search.png|Intial SPL Search Over All Time|600]]
### SIEM Duties and Roles
Setting up, maintaining, and using a SIEM creates a large amount of work that grows given an organization's size and complexity.  The responsibilities can be divided between team members whichever way makes the most sense for an organization; however, the following list of roles and responsibilities attempts to outline how some organizations might chose to divide the duties needed for the SIEM.  
- **System Administrator** - Installing the servers and configuring end points with the ingestor;
- **SOC Analyst** - Triage alerts and escalate when a threat is identified;
- **SIEM Engineer** - Develop rules, dashboards, and reports for SIEM Analysts and managers;
- **Incident Responder** - Perform containment, eradication, and recovery of security incidents; and
- **Threat Hunter** - Proactively create investigation scenarios and reviews where a threat could exist.
Each of the roles listed above will use the SIEM to accomplish their respective duties.  Some organization may have one person perform multiple roles while other organizations might rotate individuals between roles to cross train and avoid alert fatigue described earlier in the chapter.
### Available SIEM Tools
Many vendors have created and sell commercial grade SIEM solutions.  Depending on the vendor, they may offer a cloud based *software as a service (SaaS)* or an on premise environment where the servers are hosted at the customer's location and servers.  These systems can be very expensive in the hundreds of thousands of dollars for medium sized organizations up to millions of dollars for large organizations.  Some of the most popular SIEM provides include Splunk, Rapid7 InsightIDR, and Sumo Logic, although there are many others in this crowded and homogenous space.  Most of the providers offer relatively the same features but with varying user experiences.  At the time of this writing, Splunk has more than half of the market share making it one of the most attractive systems to learn on. [^1]

There are also opensource and free SIEM solutions available which require on premise installation.  The HELK stack (https://thehelk.com/intro.html) that cobles together various opensource tools including Elasticsearch, Logstash, and Kibana is a popular solution driven by community development.  Another more recent and cohesive opensource solution is Wazuh (https://wazuh.com/) which has all the features of a modern SIEM solution.  In addition, Wazuh is compatible with the opensource EDR software Ossec.
### Capabilities of a SIEM
Most SIEMs offer a range of capabilities that are fairly consistent regardless of the solution selected.  But you must ensure in advance the solution has all the features you expect before purchasing a solution.  Common features include the ability to collect large amounts of data and query it reasonably quickly.  Saving queries which can then be used as the logic for alerts and notifications increases the benefits of a SIEM.  Ideally these alerts can be marked with risk priority or severity ratings which will organize analyst workloads by focusing on the most important events first.  Many modern SIEM solutions offer some type of scripting and built-in or custom automations to respond to incidents called *security orchestration, automation, and response (SOAR)*.  Usually SOAR modules are written in Python, or some other scripting language, which performs some task.  For example, a user account compromise incident requires that the user's account be disabled and active sessions terminated - having a button next to the incident that performs these tasks on Active Directory can save crucial time.  


>[!activity] Activity 12.2 - Splunk Investigation
>Once a SIEM is up and running, with data being regularly imported, SOC analysts and incident responders use the tool to identify and investigate data breaches.  In the last activity I demonstrated the installation and ingestion of the BOTSV3 data set.  In this activity I will continue where I left off and illustrate a simple investigation and how to use SPL to query datasets.
>
> With all 2 million events matched in the botsv3 index, I scroll down to the Fields navigation on the left pane just below the timeline.  Splunk will index every record by field and summarize each field's count statistic while providing a quick link to filter just those events.  I select "host" which lists all the hosts on the network and chose the "matar" option to filter just the events related to this host.
> ![[../images/12/splunk_activity_host_field.png|Host Field Navigation|600]]
> Once selected, I observe the search field has been updated with new SPL `host=matar`.  SPL is updated using the GUI; however, a much richer experience is to write your own SPL to include operators and logic to filter and derive information from the available data.  I append `| stats count by source` to the SPL and hit enter.  This query pipes all filtered matar results to the SPL command stats where all sources are counted and displayed in the Statistics tab in the results section.
> ```SPL
> index=botsv3 host=matar | stats count by source
> ```
> ![[../images/12/splunk_activity_stats.png|Using Pipes and Commands in SPL|600]]
> This view helps me understand the types and volume of records related to the host.  Scrolling through the stats summaries I find events for stream:smtp.  SMTP, a protocol used for email, can be interesting so I select the entry and press View events which will update the SPL and show all SMTP events on the host.
> ![[../images/12/splunk_activity_smtp_filter.png|SMTP Search|400]]
> The events pane is populated with SMTP events for the matar host.  Looking at the first record I observe an odd subject line of a sent email "Fw: All your datas belong to us".
> ![[../images/12/splunk_activity_email_1.png|Email Forward Discovery|600]]
> Perhaps this user of the matar host is a victim of ransomware and this is a ransom note being forwarded.  I am curious if any other emails have a similar subject line.  The act of finding data and searching elsewhere for similar artefacts is called *pivoting*.  While still using the botsv3 index, I query `subject:"All your datas*`.  The star symbol is a wildcard which means any other characters.  
> ![[../images/12/splunk_activity_subject_search.png|Subject Line Pivot Search|600]]
> Looks like there are two hits for this subject line with the second event being the original email with a src_ip address of 104.47.34.50 - perhaps this is the attacker's IP!
> ![[../images/12/splunk_activity_srcip.png|Attacker IP Address Identified|500]]
> 
> 
> 


SIEMs usually have reporting and dashboards features powered by the query system.  A dashboard is useful to analyst and their management as it enables them to focus their attention on the types of risks that matter most.  Deriving reports from the SIEM is typically important to SOC managers to identify security and performance trends of the operation.  They can be used to justify the investment in the SIEM by clearly describing the number of malicious events detected for example.

>[!activity] Activity 12.3 - Splunk Dashboards and Reports
>SIEM solutions like Splunk will offer its users the ability to pull out information from the ingested data sets into reports, widgets, and dashboards.  Within Splunk these features are supported by the SPL where queries can be saved and used to build out visualizations and delivery of notifications and reports.  These features enable SOC analysts to monitor aggregated data at a higher level and then drill down into the data as the needs arise.  In the last two activities I demonstrated the setup and use of Splunk to conduct an investigation.  In this activity I will show how to setup reports, widgets, and dashboards within Splunk using the same BOTSV3 data set as the previous two activities.
>
>Suppose I had an interest in tracking the most frequent IP addresses found within the data sets imported into Splunk.  I could build an SPL that uses the stats command to summarize basic volume information for each host.  The host data can ben be sorted by descending order and display the top 10 results.  The following SPL accomplishes this desire which pulls the top 10 hosts by volume.
>```SPL
>index=botsv3 | stats count as cnt by host | sort cnt desc | head 10
>```
>![[../images/12/splunk_activity_top10.png|Top 10 Hosts SPL|650]]
>I won't want to retype this SPL each time I need the data.  Fortunately Splunk has a Save feature which show above the SPL field bar.  Another feature is the ability to turn a query result into a report also using the Save feature.  Pressing the Save As dropdown reveals the Report option which I select and Title the report "Top 10 Hosts" with the Time Range Picker set to Yes before hitting Save.
>![[../images/12/splunk_activity_report.png|Creating a Report From SPL|450]]
>After hitting Save I am presented with an option to View the report.  Pressing View opens the report page with my SPL data.  Anytime I want to get the latest top 10 hosts, I only have to open up this report using the Reports link in the top header.  Premium editions of Splunk can also email reports on a schedule which will be outside the scope of this activity.
>![[../images/12/splunk_activity_report_view.png|Viewing Splunk Report|650]]
>It is fairly common to create and deliver reports to stakeholders using Splunk.  Another common feature is the use of visualization gadgets and dashboards.  From a SOC analyst's point of view, have a quick reference to identify a security problem in the network promptly is highly desirable.  I can craft an SPL that will identify failed login attempts on Windows systems which could be an indicator of a brute force attack.  The Windows event 4625 is the standard event for failed logins which will be a key source of data for my query.
>```SPL
>index=botsv3 source=* EventCode=4625 | stats count as cnt
>```
>![[../images/12/splunk_activity_failed_login.png|Failed Login SPL|450]]
>The SPL yields a count of 5 failed logins presented in the Statistics tab.  The result of a query can be used in a visualization which can display the data in a more meaningful way.  For example, I can create a visualization that will help gauge if the value is low, normal, or something to be concerned about.  Clicking the Visualization tab I select the radial gauge type.
>![[../images/12/splunk_activity_visualization.png|Visualization Radial Gauge Selection|350]]
>With the radial gauge selected I can format it to distinugish low, moderate, and concerning levels of failed logins.  I choose the Format tab, Color Ranges from the left menu, and then the Manual subtab to enter ranges zero to five as green, five to ten as yellow, and ten to twenty as red.
>![[../images/12/splunk_activity_radial_settings.png|Radial Setting Configuration|400]]
>This gauge would look great in a dashboard that I can visit anytime I want to quickly reference the security standing of logins on the network.  A dashboard is meant to be regularly monitored and could even be used as a constant display in an office like a SOC for everyone to quickly view.  To get this gauge into a dashboard I choose the Save As dropdown and select New Dashboard.
>![[../images/12/splunk_activity_dashboard_create.png|Saving Gauge Into New Dashboard|600]]
>Pressing New Dashboard creates a popup window to configure the new dashboard's initial settings.  I enter "Monitoring" for the Dashboard Title and choose "Classic Dashboard" before pressing Save to Dashboard and then View Dashboard.
>![[../images/12/splunk_activity_config_dash.png|Initial Dashboard Settings|400]]
>The initial dashboard display is off to a great start but it needs more context using the title and sub headers.  I press the Edit button in the upper right corner and name the section "Brute Force" and the widget "Failed Windows Logons" then hit Save.  The final dashboard looks much clearer!
>![[../images/12/splunk_activity_final_dash.png|Final Dashboard|650]]
>I can add new visualizations, sections and change the format of the dashboard to meet my security monitoring needs and using the Dashboards tab in the top menu will let me navigate back to any of my dashboards at will.
## Threat Hunting
So far we have explored how a SOC analyst would use a SIEM to alert and investigate potential security incidents based off feeds of system events.  The SOC analyst follows a pattern of detect and respond to ensure security of networks and systems.  However, another approach to security while leveraging the power of the SIEM is **threat hunting** that proactively searches for potential cyber threats.  The role of a threat hunter, who typically works in the SOC and uses the SIEM, can be a dedicated individual, team, or a shared role.  Many SOCs rotate personnel between roles to keep team members engaged and challenged.
### Process of Hunting
Threat hunting starts with an idea or hypothesis with the assumption that a threat actor has conducted some malicious activity that has yet been discovered in the network.  This ideation phase includes studying how that attack would transpire, which technologies and protocols it would use, and consider the IoCs and artifacts that could be identified on systems.  Armed with the anticipated attacker behavior and targets, the threat hunter will next analyze systems searching for evidence of the threat.  Hopefully this results in nothing being identified, not due to a lack of quality, but due to the fact that there was no security breach!  In the final phase of threat hunting, a collection of IoCs and artifacts could be fed back into the SIEM in the form of SPLs, alerts, dashboards, and reports as prudent.  The threat hunter will typically prepare a report that describes their efforts starting from the idea, research and results of the investigation.  Of course, should the threat hunter detect a threat they would immediately escalate to the SOC's incident responders.
### Hunt Methodologies
The real art of a threat hunt centers around the initial idea phase.  Threat hunters must identify a plausible threat scenario to base their investigation on.  These ideas can be driven by a few categories of research as follows:

1. **Hypothesis Driven** - Anticipated new threat behaviors including tools, techniques, and procedures
2. **Known IoC/IaAs** - Indicators collected from existing threat behaviors and *threat intelligence* resources
3. **Advanced Analytics** - Use of big data and statistical measurements to identify anomalous activity

Really any reasonable idea could be worth exploring.  Sometimes a threat hunt can center around an organization specific activity.  For instance, an organization can plan for a sensitive data migration between vendors.  Under this circumstance, a threat hunter can design an investigation with the data transfer context in mind.  Other common threat hunting scenarios will focus on common, popular, or well publicized attacks usually centered around a powerful exploit.  A great resource for such attacks is maintained by the Cybersecurity & Infrastructure Security Agency (CISA) in their Known Exploited Vulnerabilities Catalog (https://www.cisa.gov/known-exploited-vulnerabilities-catalog).

### Threat Intelligence
Many vendors offer **threat intelligence** services that attempt to build profiles of threat actors including their tools, techniques and procedures.  This information is highly valuable to incident responders during attribution efforts - determining who compromised them.  But threat intelligence can also be a valuable resource to threat hunters during the idea and analysis phases of their investigations.  

> [!activity] Activity 12.4 - Threat Intelligence Using MITRE ATT&CK Framework
>  Perhaps the greatest resource to threat hunters developing scenarios using the known IoCs method is MITRE's ATT&CK framework (https://attack.mitre.org/).  We've covered this resource in earlier chapters which emphasized the categorization of attack tools, techniques and procedures (TTP).
>  ![[../images/12/mitre_attack.png|MITRE ATT&CK Framework Website|600]]
>  MITRE's database includes dozens of threat actor groups in the https://attack.mitre.org/groups/  page.  Many of these groups are referred to by different names depending on the organizations that track them.  Navigating to the group page I find an interesting North Korean state sponsored group Lazarus.
>  ![[../images/12/threat_activity_groups.png|Lazarus Group Summary on MITRE ATT&CK|600]]
>  Selecting the group's link reveals much more information on the threat including the campaigns that have been attributed to them.  In this campaign a list of techniques are provided that include further details on how to detect them within a network.  Aggregating several of these techniques could build a case worthy of a threat hunt!
>  ![[../images/12/threat_activity_campaign.png|Operation Dream Job Lazarus Campaign Techniques|600]]
>  The many to many nature of the ATT&CK framework affords us the ability to start with a technique to develop threat hunts off of.  Navigating to https://attack.mitre.org/techniques/enterprise/  reveals hundreds of available techniques associated with threat actors.
>  ![[../images/12/threat_activity_techniques.png|Threat Actor Techniques|600]]
>  Technique "Bypass User Account Control" T1548 at catches my eye to dive into.  I follow the technique link (https://attack.mitre.org/techniques/T1548/002/) and scroll down to the Detection section of the technique which shows several methods to detect the malicious behavior.  
>  ![[../images/12/threat_activity_detection.png|T1548 Detection Methods|600]]
>  With this information I can begin building a threat hunt and search criteria within the SIEM to detect the malicious behavior.  
## Incident Response

Incident Response Life Cycle
Plan

>[!activity] Activity 12.5 - Security Incident
>Take a moment and consider the following questions:
>1. Define a security incident
>2. Identify 3 examples of what is a security incident
>3. Identify 3 examples of what is a security event

### Phases
Preparation
Detection and analysis
Containment & eradication
Post incident recovery

>[!activity] Activity 12.6 - Splunk Enterprise Security

## Exercises

>[!exercise] Exercise 12.1 - SIEM Setup
>In this task you will install Splunk onto your Ubuntu VM, import event data, and build queries, reports, and dashboards to analyze events.
>#### Step 1 - Install Splunk Enterprise
>Using your Ubuntu VM in Bridge Adapter network mode, launch a browser and navigate to [https://www.splunk.com/en_us/download/splunk-enterprise.html](https://www.splunk.com/en_us/download/splunk-enterprise.html) and fill out the Create Account form with you CSUS email address.
>
>Upon login you should reach the download page [https://www.splunk.com/en_us/download/splunk-enterprise.html?locale=en_us](https://www.splunk.com/en_us/download/splunk-enterprise.html?locale=en_us) .  Select Linux and download the ".deb" installer.
>
>Launch a terminal and install curl and the DEB file to install Splunk Enterprise.
>```bash
>sudo apt update -y 
>sudo apt install curl -y 
>sudo dpkg -i ~/Downloads/splunk*.deb 
>```
>#### Step 2 - Setup Splunk
>Start Splunk within your launched Ubuntu VM terminal.  When launching for the first time you will be presented with the license agreement.  Press "q" and then "y" to accept the terms.  Follow the CLI questions selecting a username and password.
>```bash
>sudo /opt/splunk/bin/splunk start 
>```
>Once the setup is complete and Splunk is running, launch a web browser within your Ubuntu VM and navigate to [http://ubuntu:8000](http://ubuntu:8000/) where you'll be presented with your stand-alone instance of Splunk Enterprise.
>
>Login with the credentials you used during the setup.
>#### Step 3 - Load Data
>From within your Ubuntu VM, launch a browser and navigate to [https://github.com/splunk/botsv3](https://github.com/splunk/botsv3) and download the "BOTS V3 Dataset".  It is about 320 MBs which may take a 10 minutes or so to download.  This dataset is a curated set of logs used in Splunk's Boss of the SOC CTF challenge.
>
>Move the downloaded botsv3 data set to "/opt/splunk/etc/apps/" and unzip the contents using gunzip and tar.
>```bash
>sudo mv ~/Downloads/botsv3_data_set.tgz /opt/splunk/etc/apps/ 
>sudo gunzip /opt/splunk/tec/apps/botsv3_data_set.tgz
>sudo tar -xvf /opt/splunk/etc/apps/botsv3_data_set.tar -C  /opt/splunk/etc/apps/ 
>```
>Restart Splunk for the upload botsv3 data set/index to become available.
>```bash
>sudo /opt/splunk/bin/splunk restart 
>```
>Once restarted navigate to [http://ubuntu:8000](http://ubuntu:8000/) and navigate to Apps and then "Search & Reporting".
>
>Change the time scope to "All time" and search the term "index=botsv3" to discover all available records.  Wait a few minutes and observe millions of events loaded.
>```SPL
>index=botsv3
>```
>#### Step 4 - SPL/Query
>With all 2 million events matched in the botsv3 index, scroll down to the Fields navigation on the left pane just below the timeline.  Select "host" and chose the "matar" host.
>
>Once selected, observe the search bar now includes "host=matar" in the query.  Append "| stats count by source" to the query and hit enter.  This query pipes all filtered matar results to the SPL command stats where all sources are counted and displayed in the Statistics tab in the results section.
>```SPL
>index=botsv3 host=matar | stats count by source 
>```
>Scroll to the bottom of the Statistics page and select the "stream:smtp" and "View Events". 
>
>Review the first result in the Events pane.  The first event should be an Outlook email from Grace Hoppy with the subject "Fw: All your datas belong to us".
>
>While still using the botsv3 index, query subject:"All your datas*" and observe there are 2 hits.  The wildcard * in SPL is a placeholder for any number of characters.  Observe the second event is the original email and has a src_ip address of 104.47.34.50.
>```SPL
>index=botsv3 subject:"All your datas*"
>```
>#### Step 5 - Reports
>The following query gathers the top 10 source IP addresses by count: index=botsv3 | stats count as cnt by host | sort cnt desc | head 10.  Once the SPL is complete, press the Save As dropdown in the top right corner and select Report.
>```SPL
>index=botsv3 | stats count as cnt by host | sort cnt desc | head 10
>```
>Title the report "Top 10 Hosts", Time Range Picker as Yes, and hit Save.
>
>Once the report is created, press the View button.
>
>Review the report and observe that it can be refreshed and exported at any time for reference.
>#### Step 6 - Visualizations and Dashboards
>In this step you will develop a radial gauge visualization to enhance our dashboard. Create a new query that counts the number of failed Windows logon attempts which could identify bruteforce attacks.
>```SPL
>source=* EventCode=4625|   stats count as cnt 
>```
>Once the query is entered, select the Visualization subtab and choose the radial gauge type.
>
>With the Radial gauge selected, choose Format, Color Ranges, and change the green range to 0-5, yellow range to 6-10, and red range to 11-20. These thresholds would typically be based off normal or expected behavior over time.
>
>Now that the gauge is configured with our thresholds, select the Save As and New Dashboard.
>
>Enter the Dashboard Title as "Monitoring", select "Classic Dashboards" and press Save to Dashboard.
>
>Select the View Dashboard button and observe our Monitoring Dashboard has the radial gauge, but it excludes a title and/or context. Press the Edit button in the upper right corner and name the section "Brute Force" and name the widget "Failed Windows Logons" then hit Save.
>#### Step 7 - Challenge
>Find at least one other event worth monitoring from a security context. It doesn't have to be a Windows Event, but you can use [https://www.xplg.com/windows-server-security-events-list/](https://www.xplg.com/windows-server-security-events-list/) for inspiration. Create a query and a Visualization (your choice on type). Configure the visualization and add it to the Monitoring Dashboard with an appropriate title. 
>
>In a short paragraph, describe why you selected the query/visualization to monitor, its relevance to security, and how to interpret the information it presents.

>[!exercise] Exercise 12.2 - Splunk Enterprise Security
>In this task you will register and complete Splunk's free eLearning course "Introduction to Enterprise Security". Splunk is one of the most popular SIEM tools in the industry. Evidencing your completion of the course is a great resume builder while expanding your knowledge in security.
>#### Step 1 - Register For Course
>Navigate to [https://www.splunk.com/en_us/training/course-catalog.html?filters=filterGroup1FreeCourses](https://www.splunk.com/en_us/training/course-catalog.html?filters=filterGroup1FreeCourses) and find the "Introduction to Enterprise Security" course.
>
>Press the Register link and then press the ENROLL button.
>
>Login to Splunk using your existing account (or create one if you don't have one).
>#### Step 2 - Watch the Assigned Videos
>Once you've enrolled in the eLearning course you may start the Video coursework. Watch the videos and take notes! You can re-watch the videos at any time as many times as you'd like.
>#### Step 3 - Take the Quiz
>After the videos are complete, you are prepared for the quiz. There are 11 multiple choice questions. You are untimed and can retake the quiz as many times as you need to without penalty. You must achieve a score of 75% or greater to pass the course. Upon successful completion of the course, you must provide screenshot evidence of your Certificate which can be viewed at the top of the course page.
>
>Congrats! Consider adding your completion of this course to your LinkedIn profile and/or your resume!

[^1]: Splunk - Market Share, Competitor Insights in Security Information And Event Management (SIEM); 6 Sense; April 7th, 2024; https://6sense.com/tech/security-information-and-event-management-siem/splunk-market-share